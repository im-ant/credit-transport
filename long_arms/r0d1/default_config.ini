# ============================================================================
# Config file for training A2C agent on the Long Arms environment
#
# NOTE: the runs will be grouped together by the config file name before the
#       first "_". That is, given config files:
#           exp1_r1.ini
#           exp1_r2.ini
#           exp2_r1.ini
#       The script will generate 2 directories, exp1 and exp2, containing
#       the above three runs.
# ============================================================================


[Training]
# =====================================
# NOTE: do not change the section name or parameters, only values here.
# This section is needed to initialize the training script

# Do not think experiment name affects much other than asthetics
exp_name = long_arm
# used to also denote the run number
seed = 1
# total number of training steps  (default 9e4)
total_num_steps = 2e5

# Length of sampler trajectories. To have 1 sampler per episode, this
# quantity should be multiple of traj_length to have full traj training
sampler_batch_T = 18

# Steps between logging  (default: 128?)
log_interval_steps = 2048
# Logging mode [all, last, gap, last+gap, none]
log_snapshot_mode = none
# Log also to tensorboard summary writer?
log_use_summary_writer = True

# Do evaluation (need to test this feature more thoroughly TODO)
do_eval = False


[Env]
# Current traj_len = action_delay + corridor + 4 = 9
action_delay_len = 5
corridor_length = 0

num_arms = 2
prediction_only = True
final_obs_aliased = True
require_final_action = False
img_len = 20
grayscale = False
flatten_obs = False
scale_observation = False
dataset_path= /network/tmp1/chenant/ant/dataset/cifar/


[Algorithm]
# =====================================
# Set up RL algorithm, here specifically the "R0D1" algorithm
discount = 0.999
# 1.0 for MC, 0.0 for TD(0)
lambda_coef = 1.0

# Needs to be same as traj_length to have full traj RNN training
store_rnn_state_interval = 9
# Replay traj length, needs to be larger than rnn_interval
algo_batch_T = 12
# minibatch size for replay training (default: 64)
replay_batch_B = 16
# Buffer size
replay_buffer_size = 1e6
# Replay ratio (not sure what this means, default=1)
replay_ratio = 1
# DQN: number (update / optim) steps to update target net
target_update_interval = 1

# DQN: min step before learning?
min_steps_learn = 1e3
# DQN: number step to linearly decay eps (default 5e4)?
eps_steps = 6e4
# (in Agent) Initial epsilon (default: 1.0)
eps_init = 1.0
# (in Agent) Final epsilon (default: 0.01)
eps_final = 0.01



double_dqn = False

# Optimization parameters, below are default
learning_rate = 3e-4
clip_grad_norm = 80



[Model]
# =====================================
# Specific to the rlpyt.models.dqn.atari_r2d1_model model
use_recurrence = True

# fc between conv and lstm layers (default 512)
fc_size = 512
# (default 512)
lstm_size = 512
# (default 512)
head_size = 512
# (default False, have not tested True condition)
dueling = False



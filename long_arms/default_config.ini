# ============================================================================
# Config file for training A2C agent on the Long Arms environment
#
# NOTE: the runs will be grouped together by the config file name before the
#       first "_". That is, given config files:
#           exp1_r1.ini
#           exp1_r2.ini
#           exp2_r1.ini
#       The script will generate 2 directories, exp1 and exp2, containing
#       the above three runs.
# ============================================================================


[Training]
# =====================================
# NOTE: do not change the section name or parameters, only values here.
# This section is needed to initialize the training script

# Do not think experiment name affects much other than asthetics
exp_name = long_arm
# used to also denote the run number
seed = 1
# total number of training steps
total_num_steps = 5e5

# Steps between logging  (default: 128?)
log_interval_steps = 128
# Logging mode [all, last, gap, last+gap, none]
log_snapshot_mode = last


[Env]
# NOTE: fixed to num_arms = 2 for now, need to update in future
corridor_length = 8
num_arms = 2
img_len = 20
grayscale = False
flatten_obs = False
scale_observation = False
dataset_path= /network/tmp1/chenant/ant/dataset/cifar/


[Algorithm]
# =====================================
# Set up RL algorithm, here specifically the "R0D1" algorithm
discount = 0.997
# 1.0 for MC, 0.0 for TD(0)
lambda_coef = 1.0
use_recurrence = False

# DQN: number (update / optim) steps to update target net
target_update_interval = 1
# DQN: min step before learning?
min_steps_learn = 16
# DQN: number step to linearly decay eps?
eps_steps = 1e4

double_dqn = False

# Optimization parameters, below are default
learning_rate = 1e-4
clip_grad_norm = 80



[Model]
# =====================================
# Specific to the rlpyt.models.dqn.atari_r2d1_model model

# fc between conv and lstm layers (default 512)
fc_size = 512
# (default 512)
lstm_size = 512
# (default 512)
head_size = 512
# (default False, have not tested True condition)
dueling = False


